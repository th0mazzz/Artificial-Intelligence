Thomas Zhao
Artificial Intelligence
Period 9
March 26th 2019

Text recognition is a very complicated because there are near limitless possibilities to how said text can be presented to the computer program. In a normal world, most of the image will not even be text and will be pixels denoting a background, such is the case of a billboard sign. Therefore, crazy amounts of data is necessary for the computer program to "train" itself to be able to recognize text within an image of so much stimuli. One interesting way to generate a lot of raw training data is to take a single image and make some sort of adjustment to it, whether it is a simple rotation by a few degrees or a small cropping of the image, and use that as a new image to train upon. With this method, it is possible to acquire tons of training images, and therefore a large data set, with minimal effort (as bootleg as this method sounds).

Predictions are also an integral aspect of text recognition. When the algorithms are run multiple times, each pixel is assigned the following six things: probability that pixel is part of a word, x-coordinates and y-coordinates of a rectangle (x1y1, x2y2), and theta which describes the angle of rotation of the rectangle. With these six attributes, the program makes an inference of where the "box" is; the box being where the word likely is going to be. One may ask: what if a pixel is incorrectly stating that it is part of a word when it in fact is not? That is resolved by using the the most confident pixel. You would look at the probabilities of the pixels and use the pixel(s) with the highest probability of it being part of a word. You would penalize heavily if the algorithm missed a positive example (a positive example meaning that a pixel was indeed part of a word) much more than if it missed a negative example (if the pixel was not part of a word). This type of penalization is called high loss.
